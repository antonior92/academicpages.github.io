---
title: 'Lasso Regularization Paths for NARMAX Models via Coordinate Descent'
collection: publications
permalink: /publication/2018-ACC
excerpt: 'We propose a new algorithm for estimating NARMAX models with L1 regularization 
for models represented as a linear combination of basis functions. Due to the L1-norm penalty 
the Lasso estimation tends to produce some coefficients that are exactly zero and hence gives 
interpretable models. The proposed algorithm uses cyclical coordinate descent to compute the 
parameters of the NARMAX models for the entire regularization path and, to the best of the authors
knowledge, it is first the algorithm to allow the inclusion of error regressors in the Lasso 
estimation. This is made possible by updating the regressor matrix along with the parameter 
vector. In comparative timings we find that this modification does not harm the global efficiency
of the algorithm and can provide the most important regressors in very few inexpensive iterations.
The method is illustrated for linear and polynomial models by means of two examples.'
date: 2017-10-03
venue: 'Preprint (arXiv:1710.00598)'
paperurl: 'https://arxiv.org/abs/1710.00598'
citation: 'Ant√¥nio H. Ribeiro and Luis A. Aguirre. "Lasso Regularization Paths for NARMAX Models via Coordinate Descent" 
arXiv:1710.00598'
---

**Abstract: We propose a new algorithm for estimating NARMAX models with L1 regularization 
for models represented as a linear combination of basis functions. Due to the L1-norm penalty 
the Lasso estimation tends to produce some coefficients that are exactly zero and hence gives 
interpretable models. The proposed algorithm uses cyclical coordinate descent to compute the 
parameters of the NARMAX models for the entire regularization path and, to the best of the authors
knowledge, it is first the algorithm to allow the inclusion of error regressors in the Lasso 
estimation. This is made possible by updating the regressor matrix along with the parameter 
vector. In comparative timings we find that this modification does not harm the global efficiency
of the algorithm and can provide the most important regressors in very few inexpensive iterations.
The method is illustrated for linear and polynomial models by means of two examples.**

[Download paper here](https://arxiv.org/pdf/1710.00598.pdf)
